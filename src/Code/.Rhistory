# check that cutoffs are positive
if (!all(cutoffs>=0)) {
stop("Needs positive cutoffs!")
}
Z1_dummies[,1]<-abs(Z[,1])<cutoffs[1];
if (length(cutoffs)>1) {
for (m in (2:length(cutoffs))) {
Z1_dummies[,m]<-(abs(Z[,1])<cutoffs[m])*(abs(Z[,1])>=cutoffs[m-1]);
}
}
Z1_dummies[,length(cutoffs)+1]<-abs(Z[,1])>=cutoffs[length(cutoffs)];
} else {
Z1_dummies[,1]=Z[,1]<cutoffs[1];
if (length(cutoffs)>1) {
for (m in 2:length(cutoffs)) {
Z1_dummies[,m]=(Z[,1]<cutoffs[m])*(Z[,1]>=cutoffs[m-1]);
}
}
Z1_dummies[,length(cutoffs)+1]<-Z[,1]>=cutoffs[length(cutoffs)];
}
#calculating components of the likelihood
show(dim(Z1_dummies))
phat<-Z1_dummies%*%betap
#% %likelihoods calculated by numerical integration
#% piZ1Z2=zeros(n,1); %vector of un-truncated likelihoods
# % for i=1:n
#%     piZ1Z2(i)=2*normpdf(Z(i,1)-mudraws)'*1/sigmaZ2(i)*normpdf((Z(i,2)-mudraws)/sigmaZ2(i))/ndraws;
#% end
# %likelihoods calculated analytically
# %vector of un-truncated likelihoods
piZ1Z2=rep(0,n);
for (i in 1:n) {
Omega<-c(1+tauhat^2,tauhat^2,tauhat^2,sigmaZ2[i]^2+tauhat^2);
Omega<-matrix(Omega,2,2);
piZ1Z2[i]<-dmvnorm(c(Z[i,1], Z[i,2]),nuhat,Omega,log=FALSE)*0.5 +
dmvnorm(c(Z[i,1], Z[i,2]),-nuhat,Omega,log=FALSE)*0.5
}
# %normalizing constant
mu<-nuhat;
sigma<-sqrt(tauhat^2+1);
# %calculate probability that T statistic based on X_i has absolute value
#  %less than critval
prob_vec<-rep(0,length(cutoffs)+1);
if (symmetric==1) {
for ( m in 1:length(cutoffs)) {
prob_vec[m+1]<-0.5*(pnorm((cutoffs[m]-mu)/sigma)-pnorm((-cutoffs[m]-mu)/sigma))
0.5*(pnorm((cutoffs[m]+mu)/sigma)-pnorm((-cutoffs[m]+mu)/sigma));
}
prob_vec[length(cutoffs)+1]<-1;
mean_Z1<-prob_vec[2:(length(cutoffs)+1)]-prob_vec[1:length(cutoffs)];
} else {
for ( m in 1:length(cutoffs)) {
prob_vec[m+1]<-0.5*(pnorm((cutoffs[m]-mu)/sigma)+pnorm((cutoffs[m]+mu)/sigma));
}
prob_vec[length(cutoffs)+1]<-1;
mean_Z1<-prob_vec[2:(length(cutoffs)+1)]-prob_vec[1:length(cutoffs)];
}
normalizingconst=mean_Z1%*%betap;
#%vector of likelihoods
# %cf equation 3 in the note
L=phat*piZ1Z2/normalizingconst;
logL=log(L);
#  %objective function; note the sign flip, since we are doing minimization
LLH=-sum(log(L));
return(LLH)
}
LLH<-ReplicationAnalyticLogLikelihood(nuhat,
tauhat,
betap,
cutoffs,
symmetric,
Z,
sigmaZ2)
n<-dim(Z)[1];
n
n<-dim(Z)[1];
k<-length(betap);
betap<-as.matrix(betap,length(betap),1);
#betap<-t(betap);
Z1_dummies<-matrix(0,n,length(cutoffs)+1);
if (all(sort(cutoffs)!=cutoffs)) {
stop ("Unsorted cutoffs!")
}
set.seed(1)
cutoffs<-sort(abs(rnorm(4)))
symmetric<-1
betap<-c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
theta<-0
X<-rnorm(1)
sigma<-1
cdf<-Step_function_normal_cdf(X,theta,sigma,betap,cutoffs,symmetric)
set.seed(1)
cutoffs<-sort(abs(rnorm(4)))
symmetric<-1
betap<-c(0.1,0.2,0.3,0.4)
theta<-0
X<-rnorm(1)
sigma<-1
cdf<-Step_function_normal_cdf(X,theta,sigma,betap,cutoffs,symmetric)
set.seed(1)
cutoffs<-sort(abs(rnorm(4)))
symmetric<-1
betap<-c(0.1,0.2,0.3,0.4,0.5)
theta<-0
X<-rnorm(1)
sigma<-1
cdf<-Step_function_normal_cdf(X,theta,sigma,betap,cutoffs,symmetric)
Step_function_normal_cdf<-function(X,theta,sigma,betap,cutoffs,symmetric)
{
# %arguments:
#    %X: point at which to evaluate cdf
#  %theta: parameter value under which to evaluate cdf
#  %sigma: standard deviation of (untruncated) normal variable
#  %sigma: stdev of distribution pi of mu
#  %betap: coefficient vector for step function p, in increasing order
#  %cutoffs:vector of thresholds for step function p, in increasing order.
# %Cutoffs are given in terms of X, not z statistics
# %symmetric: dummy indicating whether publication probability is symmetric
# %around zero.  In symmetric case, cutoffs should include only positive
#%values
# %NOTE: publication probability for largest category (i.e. for those points beyond largest cutoff) normalized to one.
if (length(betap) != length(cutoffs)+1) {
stop('length of betap must be one greater then length of cutoffs');
}
# %For symmetric case, create symmetrized version of cutoffs and coefficients
if (symmetric ==1) {
# cutoffs_u <-rep(0,2*length(cutoffs));
# betap_u <-rep(0,2*length(cutoffs));
cutoffs_u<-c(-rev(cutoffs),cutoffs);
betap_u<-c(rev(betap_u),betap_u);
betap_u<-c(betap_u,1);
} else {
cutoffs_u <-cutoffs;
betap_u<-betap;
}
# %Calculate denominator in cdf
prob_vec = rep(0,length(cutoffs_u)+1);
for (m in 1:length(cutoffs_u)) {
prob_vec[m+1]=pnorm((cutoffs_u[m]-theta)/sigma);
}
prob_vec<-c(prob_vec,1);
mean_Z1<-prob_vec[2:length(prob_vec)]-prob_vec[1:(length(prob_vec)-1)];
denominator=t(mean_Z1)%*%betap_u;
cutoffs_u<-c(cutoffs_u,Inf);
if (X <= cutoffs_u[1]) {
numerator<-pnorm((X-theta)/sigma)*betap_u[1];
} else {
numerator<-pnorm((cutoffs_u[1]-theta)/sigma)*betap_u[1];
m=1;
while (X>cutoffs_u[m]) {
Xcap<-min(X,cutoffs_u[m+1]);
numerator<-numerator+(pnorm((Xcap-theta)/sigma)-pnorm((cutoffs_u[m]-theta)/sigma))*betap_u[m+1];
m<-m+1;
#  if (m > length(cut))
}
}
cdf = numerator/denominator;
return(cdf)
}
set.seed(1)
cutoffs<-sort(abs(rnorm(4)))
symmetric<-1
betap<-c(0.1,0.2,0.3,0.4,0.5)
theta<-0
X<-rnorm(1)
sigma<-1
cdf<-Step_function_normal_cdf(X,theta,sigma,betap,cutoffs,symmetric)
nuhat<-c(5,2)
tauhat<-1
betap<-c(0.1,0.2,0.3,0.4,0.5)
cutoffs<-sort(abs(rnorm(4)))
symmetric<-0
Z<-matrix(rnorm(2*100),100,2)
sigmaZ2<-rep(1,100)
#install.packages("mvtnorm")
library(mvtnorm)
ReplicationAnalyticLogLikelihood <-function(nuhat,
tauhat,
betap,
cutoffs,
symmetric,
Z,
sigmaZ2) {
#arguments: mean and stdev of distribution pi of mu
#coefficient vector for step function p, in increasing order
#vector of thresholds for step function p, in increasing order
#dummy for symmetric step function: if one, all cutoffs should be positive
#n by 2 matrix of estimates
#vector of stdevs of X2
n<-dim(Z)[1];
k<-length(betap);
betap<-as.matrix(betap,length(betap),1);
#betap<-t(betap);
Z1_dummies<-matrix(0,n,length(cutoffs)+1);
if (all(sort(cutoffs)!=cutoffs)) {
stop ("Unsorted cutoffs!")
}
if (symmetric ==1) {
# check that cutoffs are positive
if (!all(cutoffs>=0)) {
stop("Needs positive cutoffs!")
}
Z1_dummies[,1]<-abs(Z[,1])<cutoffs[1];
if (length(cutoffs)>1) {
for (m in (2:length(cutoffs))) {
Z1_dummies[,m]<-(abs(Z[,1])<cutoffs[m])*(abs(Z[,1])>=cutoffs[m-1]);
}
}
Z1_dummies[,length(cutoffs)+1]<-abs(Z[,1])>=cutoffs[length(cutoffs)];
} else {
Z1_dummies[,1]=Z[,1]<cutoffs[1];
if (length(cutoffs)>1) {
for (m in 2:length(cutoffs)) {
Z1_dummies[,m]=(Z[,1]<cutoffs[m])*(Z[,1]>=cutoffs[m-1]);
}
}
Z1_dummies[,length(cutoffs)+1]<-Z[,1]>=cutoffs[length(cutoffs)];
}
#calculating components of the likelihood
show(dim(Z1_dummies))
phat<-Z1_dummies%*%betap
#% %likelihoods calculated by numerical integration
#% piZ1Z2=zeros(n,1); %vector of un-truncated likelihoods
# % for i=1:n
#%     piZ1Z2(i)=2*normpdf(Z(i,1)-mudraws)'*1/sigmaZ2(i)*normpdf((Z(i,2)-mudraws)/sigmaZ2(i))/ndraws;
#% end
# %likelihoods calculated analytically
# %vector of un-truncated likelihoods
piZ1Z2=rep(0,n);
for (i in 1:n) {
Omega<-c(1+tauhat^2,tauhat^2,tauhat^2,sigmaZ2[i]^2+tauhat^2);
Omega<-matrix(Omega,2,2);
piZ1Z2[i]<-dmvnorm(c(Z[i,1], Z[i,2]),nuhat,Omega,log=FALSE)*0.5 +
dmvnorm(c(Z[i,1], Z[i,2]),-nuhat,Omega,log=FALSE)*0.5
}
# %normalizing constant
mu<-nuhat;
sigma<-sqrt(tauhat^2+1);
# %calculate probability that T statistic based on X_i has absolute value
#  %less than critval
prob_vec<-rep(0,length(cutoffs)+1);
if (symmetric==1) {
for ( m in 1:length(cutoffs)) {
prob_vec[m+1]<-0.5*(pnorm((cutoffs[m]-mu)/sigma)-pnorm((-cutoffs[m]-mu)/sigma))
0.5*(pnorm((cutoffs[m]+mu)/sigma)-pnorm((-cutoffs[m]+mu)/sigma));
}
prob_vec[length(cutoffs)+1]<-1;
mean_Z1<-prob_vec[2:(length(cutoffs)+1)]-prob_vec[1:length(cutoffs)];
} else {
for ( m in 1:length(cutoffs)) {
prob_vec[m+1]<-0.5*(pnorm((cutoffs[m]-mu)/sigma)+pnorm((cutoffs[m]+mu)/sigma));
}
prob_vec[length(cutoffs)+1]<-1;
mean_Z1<-prob_vec[2:(length(cutoffs)+1)]-prob_vec[1:length(cutoffs)];
}
normalizingconst=mean_Z1%*%betap;
#%vector of likelihoods
# %cf equation 3 in the note
L=phat*piZ1Z2/normalizingconst;
logL=log(L);
#  %objective function; note the sign flip, since we are doing minimization
LLH=-sum(log(L));
return(LLH)
}
LLH<-ReplicationAnalyticLogLikelihood(nuhat,
tauhat,
betap,
cutoffs,
symmetric,
Z,
sigmaZ2)
exp(1)
a<-rep(0,100)
attr(a,'true')<-rep(1,100)
a
gamlr
library(gamlr)
?gamlr
gamlr(x = rnorm(100), y = rnorm(100),subset = 1:50)
fit<-
gamlr(x = rnorm(100), y = rnorm(100),subset = 1:50)
fit
gamlr(x = rnorm(100), y = rnorm(100),subset = 1:100)
a<-rnorm(100)
b<-rnomr(100)
b<-rnorm(100)
fit1<-gamlr( a,b)
fit2<-gamlr(a,b,subset=1:50)
fit2<-gamlr(a,b,subset=1)
fit2==fit1
fit2
coef(fit2)
coef(fit1)
source('~/Dropbox (MIT)/MSR/Code/ALICE/JJFoods/RunOwnFood.R')
rm(list=ls())
# set directoryname
#directoryname<-"/bbkinghome/vsemen/ALICE"
directoryname<-"/Users/virasemenora/Dropbox (MIT)/MSR/Code/ALICE"
setwd(directoryname)
setwd("JJFoods")
# Category Lists
level1_inds_list<-list()
outname_list<-list()
# Drinks
level1_inds_list[[1]]<-7
# Meat, Fish, Poultry, Dairy
level1_inds_list[[2]]<-c(12,5,19,8)
# Non Edibles
level1_inds_list[[3]]<-c(15,11,4,26)
# Snacks, Sweeteners, Desserts, Spreads
level1_inds_list[[4]]<-c(27,6,28,25)
k<<-1
level1_inds<<-level1_inds_list[[k]]
source("Source.R")
source("RunOwnDrinks.R")
k
k<<-2
level1_inds<<-level1_inds_list[[k]]
source("Source.R")
# First Stage
# run_fs = TRUE - estimate first stage
# run_fs = FALSE - upload saved price and sales residuals
res_fs<-fs_jjfoods(run_fs=FALSE)
source("RunOwnFood.R")
own_food(k)
k
k<<-3
level1_inds<<-level1_inds_list[[k]]
source("Source.R")
source("RunOwnFood.R")
own_food(k)
k
k<<-4
level1_inds<<-level1_inds_list[[k]]
source("Source.R")
source("RunOwnFood.R")
own_food(k)
res<-res_fs
dat<-res$my_data%>%
dplyr::select(logprice,week)%>%
group_by(week) %>%
dplyr::summarise(av_price = mean(logprice[logprice>-Inf]))
q<-ggplot(dat, aes(x=week,y=av_price))+
geom_point()+
ylab("Average log price")+
theme(axis.line.x = element_line(color="black", size = 0.25),
axis.line.y = element_line(color="black", size = 0.25))+
scale_x_discrete(breaks=4*c(0:13))
print(q)
png(paste0(figdirectory,"week",as.character(level1_inds),"Price map",subset.name,".png"))
print(q)
dev.off()
source('~/Dropbox (MIT)/MSR/Code/ALICE/JJFoods/AveragePrice.R')
source('~/Dropbox (MIT)/MSR/Code/ALICE/JJFoods/AveragePrice.R')
source('~/Dropbox (MIT)/MSR/Code/ALICE/JJFoods/AveragePrice.R')
source('~/Dropbox (MIT)/MSR/Code/ALICE/JJFoods/AveragePrice.R')
source('~/Dropbox (MIT)/MSR/Code/ALICE/JJFoods/AveragePrice.R')
k<<-3
level1_inds<<-level1_inds_list[[k]]
source("Source.R")
res_fs<-fs_jjfoods(run_fs=FALSE)
# Price plots
res<-res_fs
theme_set(theme_cowplot(font_size=20))
dat<-res$my_data%>%
dplyr::select(logprice,week)%>%
group_by(week) %>%
dplyr::summarise(av_price = mean(logprice[logprice>-Inf]))
q<-ggplot(dat, aes(x=week,y=av_price))+
geom_point()+
ylab("Average log price")+
theme(axis.line.x = element_line(color="black", size = 0.25),
axis.line.y = element_line(color="black", size = 0.25))+
scale_x_discrete(breaks=4*c(0:13))
print(q)
png(paste0(figdirectory,"week",as.character(level1_inds),"Price map",subset.name,".png"))
print(q)
dev.off()
rm(list=ls())
# set directoryname
#directoryname<-"/bbkinghome/vsemen/ALICE"
directoryname<-"/Users/virasemenora/Dropbox (MIT)/MSR/Code/ALICE"
setwd(directoryname)
setwd("JJFoods")
# Category Lists
level1_inds_list<-list()
outname_list<-list()
# Drinks
level1_inds_list[[1]]<-7
# Meat, Fish, Poultry, Dairy
level1_inds_list[[2]]<-c(12,5,19,8)
# Non Edibles
level1_inds_list[[3]]<-c(15,11,4,26)
# Snacks, Sweeteners, Desserts, Spreads
level1_inds_list[[4]]<-c(27,6,28,25)
k<<-3
level1_inds<<-level1_inds_list[[k]]
source("Source.R")
rm(list=ls())
library(readr)
library(stringr)
library(dplyr)
library(plyr)
library(lubridate)
library(tidyverse)
# Set working directory
directoryname<-"/bbkinghome/vsemen/ALICE/"
directoryname<-"/Users/virasemenora/Dropbox (MIT)/MSR/Code/ALICE/"
#directoryname<-"C:/Users/t-viseme/Desktop/ALICE/"
#directoryname<-"C:/Users/mattgold/Dropbox/Code/ALICE/"
setwd(directoryname)
setwd("JJFoods/")
filename<-"PE_annon.csv"
in.name<-paste0(directoryname,'Data/', filename)
RawData <- read_delim(in.name,
delim= ",", col_types = list("i","D","i","i","c","c","c","d","d","d"))
file.name
filename
rm(list=ls())
# set directoryname
#directoryname<-"/bbkinghome/vsemen/ALICE"
directoryname<-"/Users/virasemenora/Dropbox (MIT)/MSR/Code/ALICE"
setwd(directoryname)
setwd("JJFoods")
# Category Lists
level1_inds_list<-list()
outname_list<-list()
# Drinks
level1_inds_list[[1]]<-7
# Meat, Fish, Poultry, Dairy
level1_inds_list[[2]]<-c(12,5,19,8)
# Non Edibles
level1_inds_list[[3]]<-c(15,11,4,26)
# Snacks, Sweeteners, Desserts, Spreads
level1_inds_list[[4]]<-c(27,6,28,25)
# Choose food category
# k=1: Drinks
# k=2: Dairy, Meat,Fish, Poultry
# k=3: Non Edibles
# k=4: Snacks
k<<-3
level1_inds<<-level1_inds_list[[k]]
source("Source.R")
filename
my_data<-as_tibble(read.csv(filename))
dim(my_data)
unique(my_data$Item)
length(unique(my_data$Item))
min(my_data$SalesDate)
unique(my_data$SalesDate)
min(levels(my_data$SalesDate))
max(levels(my_data$SalesDate))
dim(my_data)[1]/200
4673*3
my_data$ChannelName
my_data$SiteName
length(unique(my_data$Level3))
length(unique(my_data$Level4))
filename
rm(list=ls())
filename<-"/Users/virasemenora/Dropbox (MIT)/MSR/Code/ALICE/Data/PEAggData.csv"
## This is to fill in Matt Gold's empirical section
my_data<-as_tibble(read.csv(filename))
rm(list=ls())
# set directoryname
#directoryname<-"/bbkinghome/vsemen/ALICE"
directoryname<-"/Users/virasemenora/Dropbox (MIT)/MSR/Code/ALICE"
setwd(directoryname)
setwd("JJFoods")
# Category Lists
level1_inds_list<-list()
outname_list<-list()
# Drinks
level1_inds_list[[1]]<-7
# Meat, Fish, Poultry, Dairy
level1_inds_list[[2]]<-c(12,5,19,8)
# Non Edibles
level1_inds_list[[3]]<-c(15,11,4,26)
# Snacks, Sweeteners, Desserts, Spreads
level1_inds_list[[4]]<-c(27,6,28,25)
# Choose food category
# k=1: Drinks
# k=2: Dairy, Meat,Fish, Poultry
# k=3: Non Edibles
# k=4: Snacks
k<<-2
level1_inds<<-level1_inds_list[[k]]
source("Source.R")
# First Stage
# run_fs = TRUE - estimate first stage
# run_fs = FALSE - upload saved price and sales residuals
res_fs<-fs_jjfoods(run_fs=FALSE)
k
het<-"^(?=.*Level2|Level1)(?!.*lag)"
toplevel<-"Level1"
bottomlevel<-"Level2"
outname<-paste0("Level2",outname_list[[k]])
main(res_fs,het,outname=outname,legend_breaks=xbreaks[[k]],toplevel=toplevel,bottomlevel=bottomlevel,maxy=15,mu_bar=0.1)
het<-"^(?=.*Level3|Level2|Level1)(?!.*lag)"
toplevel<-"Level1"
bottomlevel<-"Level3|Level2"
outname<-paste0("Level3",outname_list[[k]])
res_ss_b<-main(res_fs,het,outname=outname,legend_breaks=xbreaks[[k]],toplevel=toplevel,bottomlevel=bottomlevel,maxy=25,mu_bar=0.5)
# No OLS for NonEdibles at Level4
# Average Category Elasticity at Level4
het<-"^(?=.*Level4|Level3|Level2|Level1)(?!.*lag)"
toplevel<-"Level1"
bottomlevel<-"Level4|Level3|Level2"
outname<-paste0("Level4",outname_list[[k]])
res_ss_b<-main(res_fs,het,outname=outname,legend_breaks=xbreaks[[k]],toplevel=toplevel,bottomlevel=bottomlevel,maxy=30,mu_bar=0.9)
193144
source('~/Dropbox (MIT)/MSR/Code/ALICE/JJFoods/Start.R')
